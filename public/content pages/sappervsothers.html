<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Sapper vs. Others</title>
    <!-- bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>
    <!-- icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.2/font/bootstrap-icons.css">
    <!-- jquery -->
    <script src="../plugin/jquery.min.js"></script>
    <!-- self-defined -->
    <link href="content.css" rel="stylesheet">
    <script src="content.js"></script>
</head>
<body>
<!-- navbar -->
<nav class="navbar navbar-expand-lg navbar-light my-navbar">
    <a class="navbar-brand" href="#cover"><img class="brand-logo" src="../image/graphics/logo-blue.png"></a>
    <h2 class="nav-title"><i class="bi bi-card-list"></i> Documentations </h2>
</nav>

<main>
    <div id="main-content">
        <h1 class="main-title">Prompt Sapper vs. Related Techniques</h1>
        <h2 class="main-subtitle"></h2>

        <!--img class="main-figure" src="../image/figures/SE4AIChain.jpg"-->
        <!--p class="figure-caption">Caption of the figure</p-->

        <p>
            Prompt Sapper draws inspiration from many outstanding projects and tools, but it has significant differences from them in several key aspects, including:
        </p>
        
        <ul>
            <li>
                <b>Balanced Human-AI teamwork</b>: Prompt Sapper emphasizes the collaborative interaction between artificial intelligence and human users. 
                It seamlessly marries human intelligence with artificial intelligence through AI chains, effectively addressing complex problems and achieving shared goals. 
                This human-AI team work fosters enhanced overall efficiency, reduced error rates, and empowers human users to fully harness the potential of AI. 
                This distinctive approach sets Prompt Sapper apart from existing human-driven conversational bots and AI-dominated agent frameworks, highlighting its innovative and unique value proposition.
            </li>
            <br>
            <li>
                <b>Low requirements for computing and programming skills</b>: Compared to other projects, Prompt Sapper significantly lowers the barrier to entry for creating complex AI services tailored to user needs. 
                It introduces a suite of LLM-based virtual product manager, architect, and prompt engineer to assist users in acquiring domain knowledge, analyzing task requirements, and constructing AI chains. 
                Additionally, Prompt Sapper provides an intuitive and user-friendly interface, enabling users to effortlessly interact with AI and prototype AI functionalities without the need for advanced computing or programming skills. 
                This approach broadens the specturm of people able to benefit from the advancements in artificial intelligence and underscoring the distinct position of Prompt Sapper in the AI landscape.
            </li>
            <br>
            <li>
                <b>Systematic support for AI4SE4AI</b>: Prompt Sapper values the close integration of software engineering and artificial intelligence, striving to create a systematic AI4SE4AI framework. 
                Within this framework, Prompt Sapper leverages AI technology to significantly improve the efficiency of software engineering processes, such as requirements analysis, AI chain design, construction, and testing. At the same time, Prompt Sapper adheres to and expands upon the best practices of software engineering to adapt to the new software landscape driven by AI 2.0 and Software 3.0. 
                This AI4SE4AI framework not only substantially enhances the development efficiency and project quality of AI services but also supports flexible service reuse and assembly, as well as continuous improvement and optimization of AI services to meet ever-changing demands.
            </li>
        </ul>
        
        <h2 class="main-subtitle">Prompt Sapper vs. ChatGPT (Human-Driven Conversational Bots)</h2>

        <p>
            Conversational bots (chatbots) provide a natural way for human to interact with the computer. 
            <a href="https://openai.com/blog/chatgpt">ChatGPT</a>, or other GPT-based tools such as <a href="https://github.com/features/copilot">Github Copilot</a>, <a href="https://twitter.com/codewandai?lang=en">Codewand</a>, is an impressive chatbot; however, it remains just a chatbot. 
            Its capabilities are restricted to providing answers based on the prompts it receives. 
            Thus, while it can achieve remarkable feats, including building software with natural language, it can do so only under human guidance.
        </p>
        <p>
            Such feats might lead people to mistakenly believe that "Well I will just ask ChatGPT to do it" nowadays. 
            However, they see only the impressive outcomes, overlooking the process required to achieve these results. 
            In other words, these impressive outcomes merely represent the culmination of a long process that involves substantial thinking, analysis, design, and optimization. 
            ChatGPT itself is not a silver bullet that reduces the inherent complexity of tasks, nor is it a shortcut that bypasses the process. 
        </p>
        <p>
            Although ChatGPT can provide a wealth of information and suggestions, as one end of human-AI collaboration, it remains human-driven. 
            ChatGPT's capabilities depend on human expertise, judgment, and creativity that drives the interaction process. This means people need to have a good understanding of the relevant domain and know how to effectively communicate with ChatGPT, when to consult it for what, and how to pose effective prompts and discover them. 
            However, ChatGPT itself does not directly support the systematization of this process.
        </p>
        <p>
            Unlike ChatGPT, Prompt Sapper emphasizes the importance of the analysis and design process and introduces <a href="promptmanship.html">software engineering concepts and methodology</a> to standardize and explicitly support this process. 
            Prompt Sapper integrates chatbots to support the natural human-AI interaction.
            However, it employs various intelligent co-pilots that proactively assist humans in acquiring knowledge, analyzing requirements, and designing AI chains, rather than passively following human instructions.
            
        </p>

        <h2 class="main-subtitle">Prompt Sapper vs. AI-Dominant Agent Frameworks</h2>

        <p>

            In contrast to human-driven conversational bots, AI-dominant agent frameworks sit on the opposite end of the human-AI collaboration. 
            They minimize human involvement to the greatest extent possible, allowing AI to fully control the workflow autonomously.
            Many such agent frameworks have been proposed recently, such as <a href="https://openai.com/blog/chatgpt-plugins">ChatGPT Plugins</a>, <a href="https://huggingface.co/spaces/microsoft/HuggingGPT">HuggingGPT</a>, <a href="https://github.com/microsoft/TaskMatrix">Visual-ChatGPT</a>, <a href="https://arxiv.org/abs/2303.16434">TaskMatrix.AI</a>, <a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT</a>, <a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a>, <a href="https://github.com/alvarosevilla95/autolang">autolang</a>, <a href="https://github.com/lightaime/camel">CAMEL</a>, which some people argue are the step towards Artificial General Intelligence (AGI).
        </p>
        <p> 
            Users can ask these agent frameworks to complete a task they know little or nothing about.
            Users simply express their task requirements in natural language, and the AI takes care of the rest. 
            These agent frameworks are built upon the capabilities of large language models, creating autonomous agents with diverse functionalities to collaboratively meet user needs. 
            Agents can understand or expand requirements, plan task steps, determine which model or tool to use for each step, and progressively accomplish tasks. 
        </p>
        
        <p>
            <b>Autonomy versus Interactivity, Customizability, Controllablity</b>
        </p>
        <p>
            Altnough these agent frameworks have demonstrated impressive capabilities and promising prospects, their autonomy is most suitable for scenarios with clearly defined task objectives, without the need for user interaction.
            In many experiments of these frameworks, people only care about the stunning effect of the final result, without any actual task requirements or concern for the details of the results. In other words, as long as the framework can run the task, any result is considered to meet the requirements. 
        </p>
        <p>
            Taking the <a href="https://twitter.com/pictobit/status/1645504308874563584">autolang demo</a> as an example, the user's requirement is to implement three sorting algorithms, and autolang autonomusly plan and complete the task in five steps.
        </p>
        <ul>
            <li>
                In the first step, the agent recommends three sorting algorithms, but there is no chance for the user to choose whether these three algorithms are what they need (of course, the demo author does not care about which algorithm it is). 
                In the end of this step, the agent asks the user if they need an explanation for the three algorithms, but it doesn't allow the user to input whether they need it or not, and directly proceeds to the second step. 
                This question is generated by the llm, but the actual workflow of the agent framework cannot handle the user interaction.
            </li>
            <li>
                Steps two to four involve generating code and test cases, which is also determined by the agent on its own. 
                For example, it generates three test cases without considering whether the user thinks three test cases are enough. 
                Why do the three algorithms use completely different test cases? 
                The user may not need test cases, such as when the user assigns the generated algorithm as homework for students to generate test cases. 
                In addition, the generated three algorithms are independent of each other, without considering good software engineering practices, such as suggesting a sorting interface to support programming to interface.
            </li>
            <li>
                The last step assumes that the user wants to put all three algorithms in one file, which is also entirely agent-determined. 
                However, the user may need each algorithm in a separate file, such as sending different algorithms to different students to complete the testing.
            </li>
        </ul>

        <p>
            Some people might argue that they can clarify requirements and modify results through dialogue after the generation is completed, but this is <a href="https://jina.ai/news/auto-gpt-unmasked-hype-hard-truths-production-pitfalls/">not the optimal approach from both usability and economic perspectives</a>. 
            Moreover, who knows what kind of results the agents will autonomously generate in their next run. 
            In summary, autonomy comes at the expense of interactivity, customization, and controllability.
        </p>
        <p>
            Looking at this problem from a software engineering perspective, it is normal for users to provide initial requirements that are incomplete or ambiguious for real-world applications, such as those in <a href="showcases/showcases.html">our showcases</a>. 
            A poor practice that software engineers should avoid is not interacting with users, but rather relying on their own understanding and preferences to decide what users need and build the product directly, which often results in the delivered software product not being the "right product" for the user needs, even if the product itself is flawless. Although agile development methods can quickly collect user feedback and make corrections, it ultimately wastes development resources and time. By analogy, these agent frameworks are like those poor "software engineers", who are highly capable but possibly too autonomous and thus overlooking user needs.
            Similar to software engineering, addressing issues in earlier steps can help prevent costly rework.
        </p>
        
        <p>
            <b>To be fixed: a screenshot of Design view for implmenting three sorting algorithms ...</b>
        </p>
        
        <p>
            Unlike AI-dominant agent frameworks, Prompt Sapper emphasizes understanding domain knowledge and clarifying requirement designs before construating a solution. 
            It provides user interfaces and co-pilots to support interaction and clarification with users, instead of taking the user's initial input as granted and directly constructing an AI chain solution.
            The above figure shows how Propmt Sapper interacts with the user to clarify what he actually needs for "implementing three sorting algorithms". 
            Once the user needs are clear, Prompt Sapper utilizes autonomous agents to plan the task steps and their implementation, and assembles a directly executable AI chain, while it still offers ways for users to customize and modify, considering the <a href="https://jina.ai/news/auto-gpt-unmasked-hype-hard-truths-production-pitfalls/">inadequate divide-and-conquer ability of GPT</a>.
            This type of human-AI teamwork puts human first, while keeping machine in the loop, achieving a balance between autonomy, interactivity, customization, and controllability.
        </p>
        
        <p>
            <b>Task Execution versus Software Engineering</b>
        </p>
        <p>
            
        </p>

        <h2 class="main-subtitle">Sapper vs. Agent Programming</h2>

        <p>
            With Prompt Sapper, AI chain engineers can compose prompt-based AI
            services on top of foundation models through chat-based require-
            ment analysis and visual programming. The AI services can be
            deployed as web services to provide services to end users through
            websites or mobile applications. They can also be embedded in
            large software projects or other design and development environ-
            ments (e.g, Figma) to facilitate rapid AI functionality prototyping.
            We report a variety of application cases built on Prompt Sapper,
            from coding assistant, literacy and science education, text-based
            role-playing game development, and robotic process automation.
            We envision the development direction of AI chain engineering
            (e.g., AI chain DevOps, AI service marketplace) and the enhance-
            ment of Prompt Sapper’s functionality (e.g., AI chain debugging,
            responsible AI chain engineering)
        </p>
        
        <h2 class="main-subtitle">Sapper vs. Progmpt Engineering</h2>
        <p>
            With Prompt Sapper, AI chain engineers can compose prompt-based AI
            services on top of foundation models through chat-based require-
            ment analysis and visual programming. The AI services can be
            deployed as web services to provide services to end users through
            websites or mobile applications. They can also be embedded in
            large software projects or other design and development environ-
            ments (e.g, Figma) to facilitate rapid AI functionality prototyping.
            We report a variety of application cases built on Prompt Sapper,
            from coding assistant, literacy and science education, text-based
            role-playing game development, and robotic process automation.
            We envision the development direction of AI chain engineering
            (e.g., AI chain DevOps, AI service marketplace) and the enhance-
            ment of Prompt Sapper’s functionality (e.g., AI chain debugging,
            responsible AI chain engineering)
        </p>
        
        
        <p>
            In conclusion, as a unique project, Prompt Sapper distinguishes itself from numerous excellent projects by emphasizing human-AI collaboration, lowering requirements for computing and programming skills, and providing a systematic framework for AI4SE4AI. 
        </p>
    </div>
    <div id="page-nav-id" style="display: none">ai20</div>
</main>

<!--<footer id="footer">-->
<!--</footer>-->

</body>
</html>