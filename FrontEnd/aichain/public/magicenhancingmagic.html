<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Contents</title>
    <!-- bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>
    <!-- icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.2/font/bootstrap-icons.css">
    <!-- self-defined -->
    <link href="css/content.css" rel="stylesheet">
</head>
<body>
<!-- navbar -->
<nav class="navbar navbar-expand-lg navbar-light my-navbar">
  <a class="navbar-brand" href="#cover">AI Chain</a>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link" href="#aichain">Chain</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#promptmanship">Promptmanship</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#sapper">Sapper</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#about">About Us</a>
      </li>
    </ul>
  </div>
</nav>

<main>
    <div id="main-content" class="container">
        <h1 class="main-title">Magic Enhancing Magic</h1>
        <h2 class="main-subtitle"></h2>

        <img class="main-figure" src="image/figures/Magic.jpg">
        <p class="figure-caption">Promptmanship: process, concepts, activities, and patterns</p>

        <p>
            This co-pilot activity leverages the capabilities of large language models to enhance AI chain engineering in multiple ways: <b>acquiring task knowledge</b>, <b>requirement elicitation</b>, and <b>mechanical sympathy</b>.
        </p>
        
        <h2 class="main-subtitle">Acquiring task knowledge</h2>
        <p>
            The pre-training of LLMs uses Internet-scale data, which makes the resulting models neural knowledge bases (<a href="https://arxiv.org/abs/2301.12810">Cohen et al. 2023</a>). 
            People can chat with the LLM to obtain task-related knowledge, for example, how people generally solve the task step by step, what are key challenges in problem solving, what kind of information is required or produced, are there any special conditions/cases, what are common errors and how to recover from the errors, what tool supports are available and what are their limitations.
        </p>
        
        <p>
            For example, to build <a href="">WenXiaoJie</a> (a long article writer), the engineer who consulted ChatGPT and learned that the major phases of writing a long article include: create a generic outline, personalize the outline, and iteratively expand, rewrite and polish the content.
            She also learned that the information required for these phases, for example, article theme, audience, subject matter for generic outline, favorite storyline for personalizing the outline.
            Furthermore, content consistency need to be continually checked during expansion and rewriting.
        </p>
        
        <p>
            Such task-related knowledge serve as the foundation for AI chain analysis and design.
            Although chatting with the LLM is more convenient and efficient than obtaining relevant knowledge through Internet searches, it is important to be aware of the correctness of the knowledge recommended by the LLM.
            People can verify the model's output by asking it to explain its thought process (<a href="promptdecoratorspage">Reflection</a>), list the relevant facts it knows (<a href="workerstereotypepage">Verifier</a>), and then combine it with their own knowledge to assess the validity of the model's response.
        </p>
        
        <h2 class="main-subtitle">Requirement elicitation</h2>
        <p>
            Like other software projects, AI chain engineers often start with a vague understanding of ``what they want''.
            Building software on such vague statements is a major reason for the failure of software projects.
            We need to interact with engineers through requirement elicitation and gradually clarify vague ``what they want'' statements into specific AI chain needs. 
            This challenging task can be supported by large language models, for example, through a <a href="workerstereotypepage">Reverse Questioner</a> worker. 
            By providing some examples of requirement elicitation (such as good open-ended requirement elicitation questions in <a href="https://www.amazon.com.au/Software-Requirements-Karl-Wiegers/dp/0735679665">Software Requirements</a> by Karl E. Wiegers), large language models would learn to ask good open-ended questions for specific tasks and thus elicit specific AI chain needs.
            The <a href="designviewmanualpage">Design view</a> of our <a href="https://promptsapper.tech">Sapper IDE</a> is equipped with such a requirement elicitation co-pilot that interacts with the engineer to elicit and analyze tasks requirements.
        </p>
        
        <h2 class="main-subtitle">Mechanical sympthy</h2>
        <p>
            Mechanical Sympathy means that a racing driver does not need to be a mechanical engineer, but understanding how the car works can make one a better racing driver. 
            The same applies to developing AI chains and writing prompts, especially as large language models are transforming AI from traditional design and engineering to something more akin to natural science. 
            Therefore, we need to explore and understand emergent AI behaviors and capabilities.
        </p>
        <p>
            Gwern Branwen's <a href="https://gwern.net/gpt-3#prompts-as-programming">blog</a> proposes that we need to <b>anthropomorphize prompts</b>.
            We need to consider how people would typically express the information we need if they had already discussed it on the Internet. 
            Preliminary research (<a href="https://arxiv.org/abs/2212.04037">Gonen et al. 2022</a> suggests that the more familiar the model is with the language used in the prompt, the better the prompt's performance tends to be.
            We also need to test different prompts to see what different outputs they may lead to and reverse engineer how large language models understand these prompts, thereby discovering any discrepancies between what we assume/expect and what the models understand.
            Understanding these discrepancies can guide us in decomposing tasks and writing prompts to fit with the model capability. 
        </p>
        <p>
            Research has shown that large language models can create human-level prompts (<a href="https://arxiv.org/abs/2211.01910">Zhou et al. 2023</a>),            we can also rely on large language models to create or revise prompts.
            In fact, the <a href="designviewmanualpage">Design view</a> of our <a href="https://promptsapper.tech">Sapper IDE</a> uses the LLM to create candidate prompts when generating AI chain skeleton.
            During prompt debugging in the <a href="blockviewmanualpage">Block view</a>, the engineer can also request the LLM to revise an existing prompt.
        </p>
        
    </div>
</main>

<!--<footer id="footer">-->
<!--</footer>-->

</body>
</html>